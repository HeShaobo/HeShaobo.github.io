<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>试一下代码高亮 | Gridea</title>
<link rel="shortcut icon" href="https://heshaobo.github.io/favicon.ico?v=1612709698590">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://heshaobo.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="试一下代码高亮 | Gridea - Atom Feed" href="https://heshaobo.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="redis中内存的分配由src/zmalloc.c里面的函数实现，看似只是对标准库的malloc函数做了一层简单的封装，实则大有玄机，先来看看第一个函数：zmalloc()
void *zmalloc(size_t size) {
    ..." />
    <meta name="keywords" content="" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://heshaobo.github.io">
  <img class="avatar" src="https://heshaobo.github.io/images/avatar.png?v=1612709698590" alt="">
  </a>
  <h1 class="site-title">
    Gridea
  </h1>
  <p class="site-description">
    温故而知新
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              试一下代码高亮
            </h2>
            <div class="post-info">
              <span>
                2021-02-07
              </span>
              <span>
                16 min read
              </span>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <p>redis中内存的分配由src/zmalloc.c里面的函数实现，看似只是对标准库的malloc函数做了一层简单的封装，实则大有玄机，先来看看第一个函数：zmalloc()</p>
<pre><code class="language-c">void *zmalloc(size_t size) {
    void *ptr = malloc(size+PREFIX_SIZE);   // 1
    if (!ptr) zmalloc_oom_handler(size);    // 2
#ifdef HAVE_MALLOC_SIZE
    update_zmalloc_stat_alloc(zmalloc_size(ptr));   // 3
    return ptr;
#else
    *((size_t*)ptr) = size;
    update_zmalloc_stat_alloc(size+PREFIX_SIZE);    // 4
    return (char*)ptr+PREFIX_SIZE;
#endif
}
</code></pre>
<p>step1：第一行使用了一个使用malloc申请了长度为size+PREFIX_SIZE的空间，PREFIX_SIZE是什么东西? 上文定义如下：<br>
#ifdef HAVE_MALLOC_SIZE<br>
#define PREFIX_SIZE (0)<br>
#else<br>
#if defined(__sun) || defined(<strong>sparc) || defined(<strong>sparc</strong>)<br>
#define PREFIX_SIZE (sizeof(long long))<br>
#else<br>
#define PREFIX_SIZE (sizeof(size_t))<br>
#endif<br>
#endif<br>
而HAVE_MALLOC_SIZE在src/zmalloc.h定义如下：<br>
#elif defined(<strong>APPLE</strong>)<br>
#include &lt;malloc/malloc.h&gt;<br>
#define HAVE_MALLOC_SIZE 1<br>
#define zmalloc_size(p) malloc_size(p)<br>
#endif<br>
我们先来看HAVE_MALLOC_SIZE的宏定义，如果存在__APPLE</strong>，表示这是macOS环境，macOS的malloc函数族提供了一个malloc_size函数，它可以用来返回申请过的内存空间p的大小，其他系统不提供这个方法，所以可以推断出，redis申请内存的时候多申请了PREFIX_SIZE个字节的空间，是为了用来存在该空间的大小，PREFIX_SIZE等于sizeof(size_t)即当前系统的字长，32位系统中是4字节，64位系统中是8字节<br>
step2：接着来看zmalloc的第二步，判断返回的指针如果为空，则表示申请内存失败，这时候调用zmalloc_oom_handler进行报错并且退出进程，这个不属于讨论的重点，不需要细究<br>
step3：接下来会调用update_zmalloc_stat_alloc，这个函数的作用是把申请到的空间大小记录到一个全局变量里面，以便redis能够知道当前进程使用了多少内存，update_zmalloc_stat_alloc其实是一段宏定义，其内容如下：<br>
#define update_zmalloc_stat_alloc(__n) do { <br>
size_t _n = (__n); <br>
if (_n&amp;(sizeof(long)-1)) _n += sizeof(long)-(_n&amp;(sizeof(long)-1)); <br>
if (zmalloc_thread_safe) { <br>
update_zmalloc_stat_add(_n); <br>
} else { <br>
used_memory += _n; <br>
} <br>
} while(0)<br>
字节对齐 重点看if (_n&amp;(sizeof(long)-1)) _n += sizeof(long)-(_n&amp;(sizeof(long)-1));这一句，他的作用涉及到了CPU硬件设计的知识，CPU中有一个字节对齐的概念，CPU从内存读取数据时，一次读取一个字长的数据（在32位系统中是4字节），如果不对字节进行对齐的话，则会消耗两次内存读取周期，比如说读取0x00000001，读取4字节会从0x00000001到0x00000004，而实际CPU进行读取时是对齐的，它会先读取0x00000000到0x00000003，再读取0x00000004到0x00000007,再把两次数据进行全并，这显示是一种浪费，所以字节对齐是必要的，回过头来看这行代码，_n&amp;sizeof(long)-1，32位系统中，long是4字节，long-1=3，跟_n进行与运算，其意思就是判断_n是不是以long为长度进行对齐，如果_n是对齐的，则最低两位必定为0，与运算之后结果为假，如果不对齐，则结果为真，执行_n += sizeof(long)-(_n&amp;(sizeof(long)-1))，这句又是什么意思呢？这得先解释一下malloc的行为，其实malloc内部已经给对齐进行了优化，分配返回的所有地址都是对齐过的，即使你只申请一个字节的空间，malloc返回的也是一个字长的长度，如果我们需要统计进程中申请过的堆内存空间，就必须算出这些多申请出来的空间，否则统计结果就不是精确的，(_n&amp;(sizeof(long)-1))的结果是_n低两位的值，再用sizof(long)去减掉这个值，就知道malloc多申请出来的字节数，至于这段宏为什么要用do…while包围起来？这是C编程里面的一个小技巧，可以保证宏函数不会受大括号和分号的影响，在任何地方都可以使用它（注：非常精妙的位操作算法）<br>
线程安全 经过了这个步骤，_n被修复成了真实的空间长度，接下来就使用把它加到全局变量used_memory中，不过加之前还有一个小问题，得先判断进程是不是单线程的，否则会有数据同步的问题，假如说在多线程环境中，两个线程同时写了used_memory，造成的结果是不确定的，假设线程a需要修改used_memory，它会先把它读到寄存器中，然后加上_n的值，再写回内存中，如果这时候同时有个线程b也进行这样的操作，那结果就是谁最后写到内存，数值就是谁的，因为它不是原子操作，最后的结果不是累加的，所以在加之后使用zmalloc_thread_safe这个全局变量判断是不是多线程环境，如果是的话，进入update_zmalloc_stat_add来修改used_memory的值，我们来看一下update_zmalloc_stat_add，他其实是一段宏定义：<br>
#ifdef HAVE_ATOMIC<br>
#define update_zmalloc_stat_add(__n) __sync_add_and_fetch(&amp;used_memory, (__n))<br>
#define update_zmalloc_stat_sub(__n) __sync_sub_and_fetch(&amp;used_memory, (__n))<br>
#else<br>
#define update_zmalloc_stat_add(__n) do { <br>
pthread_mutex_lock(&amp;used_memory_mutex); <br>
used_memory += (__n); <br>
pthread_mutex_unlock(&amp;used_memory_mutex); <br>
} while(0)<br>
#define update_zmalloc_stat_sub(__n) do { <br>
pthread_mutex_lock(&amp;used_memory_mutex); <br>
used_memory -= (__n); <br>
pthread_mutex_unlock(&amp;used_memory_mutex); <br>
} while(0)<br>
#endif<br>
看这个之前我们先来看看什么是HAVE_ATOMIC，它在config.h中这样定义：<br>
#if (__i386 || __amd64) &amp;&amp; <strong>GNUC</strong><br>
#define GNUC_VERSION (<strong>GNUC</strong> * 10000 + <strong>GNUC_MINOR</strong> * 100 + <strong>GNUC_PATCHLEVEL</strong>)<br>
#if (GNUC_VERSION &gt;= 40100) || defined(<strong>clang</strong>)<br>
#define HAVE_ATOMIC<br>
#endif<br>
#endif<br>
简单地说，意思就是如果在x86环境中，并且使用的编译器是GCC，他的版本高于4.01的话，或者使用的是clang编译器的话，那就定义HAVE_ATOMIC，它代表这种情况下的编译器是支持原子操作的，在gcc中，提供了__sync_add_and_fetch这样的原子操作功能（无锁编程，具体信息可查阅Built-in functions for atomic memory access），他是由编译器来支持的，__sync_add_and_fetch在实际生成汇编代码的时候，会锁住CPU的前端总线FSB，使得其他人无法对内存进行读写，这个时候对内存来说就是独占的，此时对内存的读取不会引发线程安全的问题(注：这段细节很有意思，很多语言及场景都涉及到这个东西，以后有空了再慢慢研究)<br>
线程互斥 如果编译器支持无锁编程，直接用编译器提供的__sync_add_and_fetch和__sync_sub_and_fetch来对数据进行原子增减，完成used_memory的修改，如果编译器不支持呢？势必要提供线程互斥操作，接着看：<br>
#define update_zmalloc_stat_add(__n) do { <br>
pthread_mutex_lock(&amp;used_memory_mutex); <br>
used_memory += (__n); <br>
pthread_mutex_unlock(&amp;used_memory_mutex); <br>
} while(0)<br>
他使用了互斥锁pthread_mutex_lock来进入临界区，pthread_mutex_lock是POXIS线程库提供的线程互斥锁，进入临界区之前要先使用pthread_mutex_lock取得锁，操作完之后再用pthread_mutex_unlock释放锁，进入pthread_mutex_lock时如果锁被别的线程占用，则当前线程会被挂起，直到锁释放，保护了临界区数据读写安全，redis用这种方式来保证了used_memory的安全，到这里，used_momery全局堆内存占用被正确地修改了<br>
step4：我们接着来看，如果HAVE_MALLOC_SIZE没被定义，则说明当前环境下的malloc函数族是不支持根据申请到的指针来反查内存大小的，这时候先用*((size_t*)ptr) = size;把当前调用者申请的空间大小size记录到指针最开始的地方（因为之前申请的时候空间是加了一个PREFIX_SIZE的长度的，多的这个长度就用来放size，以利redis想查询该内存块空间的时候方便获取），接着同样调用update_zmalloc_stat_alloc(size+PREFIX_SIZE); 来增加到used_memory的值，记录这时候他是把PREFIX_SIZE的长度也算进去的，因为used_memory要统计的是真实精确的空间消耗，到了这里，申请也结束了，向用户返回(char*)ptr+PREFIX_SIZE;，最开始的空间已经被用来记录内存块size，所以返回的地址需要从指针递增PREFIX_SIZE个长度算起<br>
zcalloc，zrealloc，zfree原理上与zmalloc相同，不再单独分析</p>
<ol>
<li>跟info命令相关<br>
当我们在redis-cli中使用info memory命令的时候，返回的是一个关于redis内存方面的信息，例如：</li>
</ol>
<h1 id="memory">Memory</h1>
<p>used_memory:1019824<br>
used_memory_human:995.92K<br>
used_memory_rss:2174976<br>
used_memory_rss_human:2.07M<br>
used_memory_peak:1020640<br>
used_memory_peak_human:996.72K<br>
total_system_memory:8589934592<br>
total_system_memory_human:8.00G<br>
used_memory_lua:37888<br>
used_memory_lua_human:37.00K<br>
maxmemory:0<br>
maxmemory_human:0B<br>
maxmemory_policy:noeviction<br>
mem_fragmentation_ratio:2.13<br>
mem_allocator:libc<br>
used_memory这个很好理解，它就是我们上文提到的那个used_memory，进程实际申请过的内存空间大小，used_memory_human则是把used_memory_human转换了一个格式，变成对用户友好的方式显示<br>
接下来主要说的是used_memory_rss这一项，它表示的是系统分配给进程的空间大小，但是不包含被swap到磁盘的内容，即是说当前进程在内存空间中数据段占用的大小，这个值对性能分析很有用，后面再说，先说一下used_memory_rss的计算方法，在zmalloc.c中：<br>
size_t zmalloc_get_rss(void) {<br>
int page = sysconf(_SC_PAGESIZE);<br>
size_t rss;<br>
char buf[4096];<br>
char filename[256];<br>
int fd, count;<br>
char *p, <em>x;<br>
snprintf(filename,256,&quot;/proc/%d/stat&quot;,getpid());<br>
if ((fd = open(filename,O_RDONLY)) == -1) return 0;<br>
if (read(fd,buf,4096) &lt;= 0) {<br>
close(fd);<br>
return 0;<br>
}<br>
close(fd);<br>
p = buf;<br>
count = 23; /</em> RSS is the 24th field in /proc/<pid>/stat */<br>
while(p &amp;&amp; count--) {<br>
p = strchr(p,' ');<br>
if (p) p++;<br>
}<br>
if (!p) return 0;<br>
x = strchr(p,' ');<br>
if (!x) return 0;<br>
*x = '\0';<br>
rss = strtoll(p,NULL,10);<br>
rss *= page;<br>
return rss;<br>
}<br>
系统分配给进程实际上是以分页的形式给出，比如在32位Linux系统中，每个内存分页是4KB，通过页表来将进程线性地址与这些物理分页进行映射，在Linux中，/proc/<pid>/stat中记录的是进程信息，其实第24项是rss（当前进程在内存空间中占用的分页数量，不包含被swap出去的空间）<br>
int page = sysconf(_SC_PAGESIZE);获取的是当前系统中分页的大小，接下来通过读取/proc/<pid>/stat文件，定位到rss项，取得当前进程在用的分页数量，跟page相乘则是当前系统分配的空间大小<br>
mem_fragmentation_ratio 这一项表示的是内存碎片率，但是我觉得并不是碎片的意思，他的计算方法如下：<br>
float zmalloc_get_fragmentation_ratio(size_t rss) {<br>
return (float)rss/zmalloc_used_memory();<br>
}<br>
意思就是以当前进程当前内存空间大小除以实际申请过的内存数量，正常情况下rss总会比used_memory高，但是如果系统内存负载过高，导致进程的部分内存被swap到磁盘，这时候rss就会小于used_memory，结果就是mem_fragmentation_ratio的值小于1<br>
Tips: mem_fragmentation_ratio小于1说明进程被swap了，对于redis这种依赖于内存的缓存服务来说，这会严重影响性能，说明你的服务该优化了<br>
3. copy-on-write 写时复制相关<br>
UNIX中，父进程使用fork()生成子进程的时候，会把父进程的内存空间拷贝出一个副本给子进程，使得子进程在初始化的时候拥有跟父进程一样的内容，虽然对开发者来说很友好，但是会带来一个严重的性能问题，假如父进程占用内存很大，比如好几个G，这个时候fork()需要内核把这么大的内存做一个拷贝，对内存和CPU来说都是一个很重的操作，所以Linux内核支持了写时复制技术，在fork()的之后，子进程跟父进程共享同一份内存页表，即两个进程各自拥有自己的页表，但是这些页表映射到同一份物理页框上面，内核不需要再进行复制操作，同时会把这些页框设置为只读状态，共享页框的这些进程如果要进入写入操作，则再由内核拷贝一个副本出来，并修改该进程的页表映射，完成私有化，这极大地提升了系统性能<br>
redis会定时把内存中的数据写入到磁盘，使用了两种技术，RDB和AOF，RDB是把整个redis的数据全量写入到磁盘中，AOF则类似于mysql的binlog，里面只记录redis-server的写入命令，使用AOF文件进行恢复的时候再把这些命令读出来重放到redis-server中，这里我们使用RDB来讨论他跟copy-on-write的关系<br>
RDB支持redis-server定时将内存中的数据持久化备份到磁盘，启动备份的时候，会由主进程fork出一个RDB进程，由RDB进程来将数据写入到磁盘，可以想象，如果redis-server内存占用很高的时候，普通fork()将会使内存占用翻倍而用cpu时间会消耗在数据复制上，在Linux上支持的写时复制技术，可以使创建子进程的成本降到很低，而且RDB进程仅进行数据读取，不会去修改db数据，不需要内核频繁做页表拷贝，会导致子进程内存占用上涨的情况是父进程对数据进行了修改，一量修改了某个页表，页表将会被拷贝<br>
在RDB进程备份完成后会在log中记录：RDB: xx MB of memory used by copy-on-write，意思是在本次备份过程中子进程产生了多少私有内存空间，这个统计值是由zmalloc.c中的zmalloc_get_private_dirty计算出来的，看一下它的实现：</p>
<pre><code class="language-c">size_t zmalloc_get_private_dirty(void) {
    char line[1024];
    size_t pd = 0;
    FILE *fp = fopen(&quot;/proc/self/smaps&quot;,&quot;r&quot;);
    if (!fp) return 0;
    while(fgets(line,sizeof(line),fp) != NULL) {
        if (strncmp(line,&quot;Private_Dirty:&quot;,14) == 0) {
            char *p = strchr(line,'k');
            if (p) {
                *p = '\0';
                pd += strtol(line+14,NULL,10) * 1024;
            }
        }
    }
    fclose(fp);
    return pd;
}
</code></pre>
<p>读取/proc/self/smaps，这个文件是linux提供用来记录进程内存布局的，strncmp(line,&quot;Private_Dirty:&quot;,14)则是获取smaps中私有脏页的大小，表示的已被改写的私有页面，对应的是RDB子进程在备份过程中产生的内存空间<br>
Tips：如果redis主进程在非常繁忙地进行数据修改工作，这时候RDB子进程可能会产生大量的Private_Dirty，这是实际应用场景中需要考虑到的事情<br>
4. malloc库<br>
redis支持多种malloc库，除了glibc的标准malloc库，还有google的tcmalloc，facebook的jemalloc，可以在编译的时候进行指定，各种库的优劣对比以及原理跟redis关系不大，这里就不写了，以后有空再专门研究一下<br>
end<br>
2019/09/27 更新<br>
在继续学习redis源码的过程中，我发现redis服务过程中会大量创建/销毁各种小内存的对象，比如像链表节点这样的节点，而这些完全都是通过zmalloc来实现的，这些内存分配回收工作全部交给了malloc来实现，如果在malloc上面再实现一层slab缓存（比如memcache、linux内核就是这样实现的），会不会提高性能呢，带着这个问题我去redis的github issue上面搜索了一下，果然有人在提问，传送门，作者回复如下：<br>
Hello, we now use jemalloc by default. It internally implements the allocation of many small objects using slab allocators and other similar stuff that we’ll surely implement in a less interesting way compared to jemalloc itself 😉<br>
So I guess this is not a good idea. Also from profiling we know that our bottleneck is not in the allocator… and maybe what we could do to dramatically improve on that is adding support for stack allocated objects when they can do a difference.<br>
意思就是说redis默认使用的jemalloc库已经实现了这种效果，而且redis本身的瓶颈并不在这里。<br>
等之后我再专门做一些jemalloc的分析<br>
WRITTEN BY</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#memory">Memory</a></li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://heshaobo.github.io/post/hello-gridea/">
              <h3 class="post-title">
                Hello Gridea
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://heshaobo.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
